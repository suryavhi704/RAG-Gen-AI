# ğŸ§  RAG-Based Text-to-Text Generation using LangChain

## ğŸ” Overview
This project demonstrates an end-to-end implementation of Retrieval-Augmented Generation (RAG) for intelligent Q&A using LangChain, ChromaDB, and Gradio. The system retrieves contextual documents and generates human-like responses via a connected LLM pipeline.

## ğŸ“ Project Structure
ğŸ“¦rag-langchain-project
â”œâ”€â”€ data/ # Input documents
â”œâ”€â”€ embeddings/ # Generated embedding vectors
â”œâ”€â”€ vector_store/ # ChromaDB files
â”œâ”€â”€ app.py # Gradio GUI and LangChain RAG integration
â”œâ”€â”€ utils/ # Helper functions
â””â”€â”€ README.md

## âš™ï¸ Technologies Used
- LangChain  
- ChromaDB  
- Hugging Face Transformers  
- Gradio  
- Python 3.10+  

## ğŸ” Pipeline Steps
1. Data loading and pre-processing  
2. Embedding generation  
3. Vector storage using Chroma  
4. RAG chain linking with LLM  
5. GUI frontend via Gradio  

# ğŸ“¸ Sample Output
User Input: "What is Ai?"
Response: "Ai has emerged as powerful engines  of innovation."

# ğŸ“š Acknowledgments
This project was developed under the guidance of an IIT Indore expert as part of the Intellipaat AI and Data Science course.
