# Import necessary libraries
from langchain_chroma import Chroma
from langchain_core.prompts import PromptTemplate
from langchain_text_splitters import CharacterTextSplitter
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers.string import StrOutputParser
from langchain_huggingface import HuggingFaceEmbeddings
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
from langchain import HuggingFacePipeline
import gradio as gr

# Load and prepare your knowledge base (replace with your actual data loading)
with open('/content/IMPORTANCE OF AI.txt') as f:
    files = f.read()

# Text Chunking
text_split = CharacterTextSplitter(
    chunk_size=1000,  # number of characters into a chunk
    chunk_overlap=200,  # common part between i and i-1 chunk.(to maintain the consistancy)
    length_function=len,
)
text = text_split.create_documents([files])

# Create Vector Embeddings
# Install necessary libraries if not already installed
# !pip install sentence-transformers langchain_huggingface

embedding_model = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')

# Create a Vector Database
vectorDB = Chroma(
    collection_name="suryavhi",
    embedding_function=embedding_model
)

# Load the documents into the DB
import asyncio
storage = asyncio.run(vectorDB.aadd_documents(text))


# Create Retriever
retriever = vectorDB.as_retriever()

# Load & Configure LLM
# Install transformers if not already installed
# !pip install transformers

tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-base')

if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})

model = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-base')
model.resize_token_embeddings(len(tokenizer))
model.config.pad_token_id = tokenizer.pad_token_id


generator = pipeline(
    'text2text-generation',
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=200
)

# Suppress the LangChainDeprecationWarning
import warnings
warnings.filterwarnings("ignore", category=LangChainDeprecationWarning)

LLM = HuggingFacePipeline(pipeline=generator)


# Custom Prompt
template = """Use the context provided to answer the question. If you dont know, then say you dont know
Context:
{context}
Question:
{question}
Answer:
"""
custom_template = PromptTemplate(template=template)

# Connect Everything (The RAG Chain)
rag_chain = (
    {'context': retriever, 'question': RunnablePassthrough()}
    | custom_template
    | LLM
    | StrOutputParser()
)

# --- Gradio Chat UI ---

# Function to handle the chat interaction
def chat(message, history):
    # Invoke the RAG chain with the user's message
    bot_message = rag_chain.invoke(message)
    # Append the user message and bot response to the history
    history.append((message, bot_message))
    return history, history

# Function to clear the chat history
def clear_chat():
    # Return empty lists for chat history
    return [], []

# Custom CSS for styling the Gradio interface
custom_css = """
.chatbot {
    background-color: #f0f2f5;
    padding: 10px;
    border-radius: 15px;
    font-family: 'Segoe UI', sans-serif;
}

.message.user {
    background-color: #d9f1ff;
    color: #000;
    align-self: flex-end;
    border-radius: 15px 15px 0 15px;
    padding: 10px 14px;
    max-width: 70%;
    margin: 5px;
    display: inline-block;
}

.message.bot {
    background-color: #e4e6eb;
    color: #000;
    align-self: flex-start;
    border-radius: 15px 15px 15px 0;
    padding: 10px 14px;
    max-width: 70%;
    margin: 5px;
    display: inline-block;
}

#textbox {
    border-radius: 10px;
    padding: 10px;
    border: 1px solid #ccc;
}

#clear-button {
    background-color: #333;
    color: white;
    padding: 6px 12px;
    border-radius: 5px;
}
"""

# Function to render chat messages with custom classes for styling
def render_chat(history):
    chat_html = ""
    for user, bot in history:
        chat_html += f"<div class='message user'>{user}</div><br>"
        chat_html += f"<div class='message bot'>{bot}</div><br>"
    return chat_html

# Build the Gradio interface using gr.Blocks
with gr.Blocks(css=custom_css) as demo:
    # Add a title to the interface
    gr.Markdown("<h2 style='text-align: center; color: #004c99;'>Conversational AI Chatbot</h2>")

    # Create a column for the chat elements with custom styling
    with gr.Column(elem_classes="chatbot"):
        # HTML element to display the chat messages
        chatbox = gr.HTML()
        # Textbox for user input
        msg = gr.Textbox(placeholder="Type your message here...", elem_id="textbox", show_label=False)
        # Button to trigger the chat
        clear = gr.Button("Enter And wait", elem_id="clear-button")

    # State variable to store the chat history
    history = gr.State([])

    # Function to handle user input and update the chat display
    def user_chat(message, history):
        # Get the bot's response
        bot_response = rag_chain.invoke(message)
        # Append the user message and bot response to the history
        history.append((message, bot_response))
        # Render the updated chat history as HTML
        return render_chat(history), history

    # Event listener for submitting the message when Enter is pressed
    msg.submit(user_chat, [msg, history], [chatbox, history])
    # Event listener for the clear button (clears the chat history)
    clear.click(lambda: ("", []), None, [chatbox, history], queue=False)

# Launch the Gradio demo
demo.launch(share=True)